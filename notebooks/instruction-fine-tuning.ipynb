{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"bf9b55dbc6c74b00863ba044c70e2afb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_488326a6d9a84eaba38762b5ae04a14b","IPY_MODEL_b7873bb87304495aae8e91c8a35e381f","IPY_MODEL_3f586db3fbf349b682d762b61eb3abf6"],"layout":"IPY_MODEL_b93c588334de46318cec69b40312f08c"}},"488326a6d9a84eaba38762b5ae04a14b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f0a73c0f2b954f659bceefe40b2c57c7","placeholder":"​","style":"IPY_MODEL_a12e6efbaeb44fbf90f4418fb239295f","value":"tokenizer_config.json: 100%"}},"b7873bb87304495aae8e91c8a35e381f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3bd0930a40234ebbb5018a08f0cb6b14","max":1177,"min":0,"orientation":"horizontal","style":"IPY_MODEL_186ef8fe3f604eca8f12b122316ea4b8","value":1177}},"3f586db3fbf349b682d762b61eb3abf6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef6d0da92a074e559bb87e2a5d9eff3a","placeholder":"​","style":"IPY_MODEL_706c5e5fb17f4e8ab08c819c0efed020","value":" 1.18k/1.18k [00:00&lt;00:00, 38.0kB/s]"}},"b93c588334de46318cec69b40312f08c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f0a73c0f2b954f659bceefe40b2c57c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a12e6efbaeb44fbf90f4418fb239295f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3bd0930a40234ebbb5018a08f0cb6b14":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"186ef8fe3f604eca8f12b122316ea4b8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ef6d0da92a074e559bb87e2a5d9eff3a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"706c5e5fb17f4e8ab08c819c0efed020":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bedc0ee76dd040539757450d09e58a77":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a4e882f06c1c4a2385c288108f0481a1","IPY_MODEL_6216c977f37d44308ca776ccc48adc10","IPY_MODEL_cf1e001248c14e37a791ef2626fcaa4f"],"layout":"IPY_MODEL_992dbe41de2d46d4add67f373140bda7"}},"a4e882f06c1c4a2385c288108f0481a1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2617d84b4c184e50b04913e558033c5e","placeholder":"​","style":"IPY_MODEL_6d8160d8eb6e4f7cb136cd9ec75b6df3","value":"vocab.json: 100%"}},"6216c977f37d44308ca776ccc48adc10":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_49c60cda0e2b462eb0ad5bd2496f402b","max":999186,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c5e7ce406fdd45ecb896ec7b52c20316","value":999186}},"cf1e001248c14e37a791ef2626fcaa4f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_523b9b896b9f4d01b95fe9140f386934","placeholder":"​","style":"IPY_MODEL_03c9b7760e4641ae8d69c6b9f74a9413","value":" 999k/999k [00:00&lt;00:00, 5.34MB/s]"}},"992dbe41de2d46d4add67f373140bda7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2617d84b4c184e50b04913e558033c5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d8160d8eb6e4f7cb136cd9ec75b6df3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"49c60cda0e2b462eb0ad5bd2496f402b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5e7ce406fdd45ecb896ec7b52c20316":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"523b9b896b9f4d01b95fe9140f386934":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03c9b7760e4641ae8d69c6b9f74a9413":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"78a637dee950439f869516018a484eda":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5c0beefd39764eccb5ba61bc44959c86","IPY_MODEL_6ad31123f2c34165bd7d3cbca6b618c7","IPY_MODEL_172f7d703ed34bc9a5e3d1b3ff6f25fc"],"layout":"IPY_MODEL_9abf1e18df074bfba5fdccfc8f6216f8"}},"5c0beefd39764eccb5ba61bc44959c86":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_895dcc48f7aa44b5b573666a64769804","placeholder":"​","style":"IPY_MODEL_32d5039d0f1749b9a7250ebb4900f6dd","value":"merges.txt: 100%"}},"6ad31123f2c34165bd7d3cbca6b618c7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4da23bc3a9eb4e84b7c9ed8080568048","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_072d0e8322a34dea96554d6252091459","value":456318}},"172f7d703ed34bc9a5e3d1b3ff6f25fc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c4e00699a1374c39b5b7ef5df6b8e499","placeholder":"​","style":"IPY_MODEL_1e7e1a25282e42a08b724477db226398","value":" 456k/456k [00:00&lt;00:00, 14.2MB/s]"}},"9abf1e18df074bfba5fdccfc8f6216f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"895dcc48f7aa44b5b573666a64769804":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32d5039d0f1749b9a7250ebb4900f6dd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4da23bc3a9eb4e84b7c9ed8080568048":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"072d0e8322a34dea96554d6252091459":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c4e00699a1374c39b5b7ef5df6b8e499":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e7e1a25282e42a08b724477db226398":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7e6113f3e075462a9598543604cfb8cd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f6bc4842267c42d49f7b23e8ada157a3","IPY_MODEL_db2b2c9225134542b33f6069b5484f93","IPY_MODEL_474514b634524ab2a014fe95ac178023"],"layout":"IPY_MODEL_ccbc1b56e26441508d3d0dbd0ed3094b"}},"f6bc4842267c42d49f7b23e8ada157a3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_500848552ab54edf8995d9b36b34dbad","placeholder":"​","style":"IPY_MODEL_4ead01e004f045da8db61fb6de89b3c4","value":"tokenizer.json: 100%"}},"db2b2c9225134542b33f6069b5484f93":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5d5c949a4d1486e8fbc061a28a1b9d6","max":3557680,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d14332d301d243418db0ae6e6524fe9e","value":3557680}},"474514b634524ab2a014fe95ac178023":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_db39386848ac49fd811cf4989b8a3fc7","placeholder":"​","style":"IPY_MODEL_8bf16978f7644bd38ee79cf4204dd528","value":" 3.56M/3.56M [00:00&lt;00:00, 20.4MB/s]"}},"ccbc1b56e26441508d3d0dbd0ed3094b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"500848552ab54edf8995d9b36b34dbad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ead01e004f045da8db61fb6de89b3c4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b5d5c949a4d1486e8fbc061a28a1b9d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d14332d301d243418db0ae6e6524fe9e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"db39386848ac49fd811cf4989b8a3fc7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8bf16978f7644bd38ee79cf4204dd528":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3968a8dde9aa43ee8cdee528ceede631":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1696fd0d30f24d7b98d5b5c3d6f64c04","IPY_MODEL_c1af5781bc4b4cb388bb33846a608a3f","IPY_MODEL_a024b9977a8d42a78bc5db188eca44b1"],"layout":"IPY_MODEL_8c6d6d5e033e4154a0c16b8a01bb72d1"}},"1696fd0d30f24d7b98d5b5c3d6f64c04":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc3e8f7c5c674902bf25620ce4b110ed","placeholder":"​","style":"IPY_MODEL_5f39eae18a8e498f8c400892da5202d1","value":"added_tokens.json: 100%"}},"c1af5781bc4b4cb388bb33846a608a3f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_98e7e3cd0e7041fd960836dc537f68be","max":69,"min":0,"orientation":"horizontal","style":"IPY_MODEL_85a0d50204cc4e21a357fbd9598952a7","value":69}},"a024b9977a8d42a78bc5db188eca44b1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9af3bdcc000e4cb2aee148d1818cff8e","placeholder":"​","style":"IPY_MODEL_24a6b8ac869847f2bf2a5036d3299e4c","value":" 69.0/69.0 [00:00&lt;00:00, 3.08kB/s]"}},"8c6d6d5e033e4154a0c16b8a01bb72d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc3e8f7c5c674902bf25620ce4b110ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f39eae18a8e498f8c400892da5202d1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"98e7e3cd0e7041fd960836dc537f68be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"85a0d50204cc4e21a357fbd9598952a7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9af3bdcc000e4cb2aee148d1818cff8e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24a6b8ac869847f2bf2a5036d3299e4c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"08f860e687354f79aae73fd08cad9a8c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_45078fcea9494334a2387578fe23e9dc","IPY_MODEL_53a45b78c5ca44ec897191af92ae0d26","IPY_MODEL_ce59e36173ac4e2da25e5cbc52ca00c7"],"layout":"IPY_MODEL_38f1e2a13eaa4d708fff042c9d01a943"}},"45078fcea9494334a2387578fe23e9dc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f2b63882be7a43cb8ca491fc5914bed2","placeholder":"​","style":"IPY_MODEL_952b43b229c8419e9652512118d2bbe1","value":"special_tokens_map.json: 100%"}},"53a45b78c5ca44ec897191af92ae0d26":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_78c3b7bc8dac4cb6b6a87fef366469f6","max":658,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f486de8cad6a4a2481a22ea50cdc6ddb","value":658}},"ce59e36173ac4e2da25e5cbc52ca00c7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f51e8ff81c54816b7755eed1b228dc1","placeholder":"​","style":"IPY_MODEL_368bb8fa3b334ea48459740be3ded981","value":" 658/658 [00:00&lt;00:00, 18.9kB/s]"}},"38f1e2a13eaa4d708fff042c9d01a943":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2b63882be7a43cb8ca491fc5914bed2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"952b43b229c8419e9652512118d2bbe1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"78c3b7bc8dac4cb6b6a87fef366469f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f486de8cad6a4a2481a22ea50cdc6ddb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8f51e8ff81c54816b7755eed1b228dc1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"368bb8fa3b334ea48459740be3ded981":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"077d6164f791459da37bd2ca39af68cb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9d6719865c4b4fd0acf28531231a6211","IPY_MODEL_54dad0613455487aad5697875763b5e0","IPY_MODEL_d106813c4b3546938cd5cd3612602df9"],"layout":"IPY_MODEL_17d58507e8bf4823a2b6db873cbc964a"}},"9d6719865c4b4fd0acf28531231a6211":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_62cf75a07f1a4ebbb5c464bed2303858","placeholder":"​","style":"IPY_MODEL_aad16eaba4b34ac8a50fa4a9c4f335d7","value":"config.json: 100%"}},"54dad0613455487aad5697875763b5e0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7eaa9d30e95743c1a261c06a55ba478c","max":915,"min":0,"orientation":"horizontal","style":"IPY_MODEL_82ee1e5579df4e3a9855218cde937904","value":915}},"d106813c4b3546938cd5cd3612602df9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7fb3a2101b774be98f842683d7f989c7","placeholder":"​","style":"IPY_MODEL_4721ea1b70ed4f9dae8efd41d6ba674e","value":" 915/915 [00:00&lt;00:00, 97.0kB/s]"}},"17d58507e8bf4823a2b6db873cbc964a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62cf75a07f1a4ebbb5c464bed2303858":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aad16eaba4b34ac8a50fa4a9c4f335d7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7eaa9d30e95743c1a261c06a55ba478c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82ee1e5579df4e3a9855218cde937904":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7fb3a2101b774be98f842683d7f989c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4721ea1b70ed4f9dae8efd41d6ba674e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"693f359f1fb34d5a8d100a6c4fa6717f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_83112588599c438590e6018174696aa6","IPY_MODEL_bea8a00600c64b4c8a070b52961cda4c","IPY_MODEL_4b680ff83e584834b459e1646d8a31f6"],"layout":"IPY_MODEL_a940428ab0c44387b7c5f341f5c3c65f"}},"83112588599c438590e6018174696aa6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6ad7a7a2ad944270898b5000495b0c45","placeholder":"​","style":"IPY_MODEL_c31468a293e2438d953d473c086be0d3","value":"model.safetensors: 100%"}},"bea8a00600c64b4c8a070b52961cda4c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ab9a5aa476545cb829eba797abb8b44","max":3096181288,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ab3248abc15f4c1db2baf80c7f0225e8","value":3096181288}},"4b680ff83e584834b459e1646d8a31f6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2bb3fad9c888466c9794003523f7faa2","placeholder":"​","style":"IPY_MODEL_174d8f941b80479db210f010360794d7","value":" 3.10G/3.10G [00:44&lt;00:00, 63.9MB/s]"}},"a940428ab0c44387b7c5f341f5c3c65f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ad7a7a2ad944270898b5000495b0c45":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c31468a293e2438d953d473c086be0d3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1ab9a5aa476545cb829eba797abb8b44":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab3248abc15f4c1db2baf80c7f0225e8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2bb3fad9c888466c9794003523f7faa2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"174d8f941b80479db210f010360794d7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4a920f610b624d3b988e67cf57c840e7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_90f75ad85a5a4e3182d107bccc55df7c","IPY_MODEL_e4498f3949e7430096b868f514069833","IPY_MODEL_90c11446aed04882b67fbd7c7ba5a39a"],"layout":"IPY_MODEL_902d1677eb8e40068606c74c69d52040"}},"90f75ad85a5a4e3182d107bccc55df7c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d9ead07f4b54481b9f3af19db19911c","placeholder":"​","style":"IPY_MODEL_6a06d9e2d271492990afb0689130d7e1","value":"generation_config.json: 100%"}},"e4498f3949e7430096b868f514069833":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3568fc179b9948de9cb066226d846108","max":119,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a38092868574472a8430a57c5226c4fc","value":119}},"90c11446aed04882b67fbd7c7ba5a39a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a86f885cdea14a44843b8411b1b22e28","placeholder":"​","style":"IPY_MODEL_4a3a27ecd5f54d64887637734c6c709e","value":" 119/119 [00:00&lt;00:00, 10.1kB/s]"}},"902d1677eb8e40068606c74c69d52040":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d9ead07f4b54481b9f3af19db19911c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a06d9e2d271492990afb0689130d7e1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3568fc179b9948de9cb066226d846108":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a38092868574472a8430a57c5226c4fc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a86f885cdea14a44843b8411b1b22e28":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a3a27ecd5f54d64887637734c6c709e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11742668,"sourceType":"datasetVersion","datasetId":7371464},{"sourceId":11743544,"sourceType":"datasetVersion","datasetId":7372041}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport numpy as np\nimport pandas as pd\nimport tiktoken\nimport torch\nimport torch.nn as nn\nimport tiktoken\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import GPT2LMHeadModel\n","metadata":{"id":"4ve5dRggxxBT","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T12:35:58.797072Z","iopub.execute_input":"2025-05-18T12:35:58.797349Z","iopub.status.idle":"2025-05-18T12:36:23.268902Z","shell.execute_reply.started":"2025-05-18T12:35:58.797330Z","shell.execute_reply":"2025-05-18T12:36:23.268146Z"}},"outputs":[{"name":"stderr","text":"2025-05-18 12:36:12.094209: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747571772.304070      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747571772.365303      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n        super().__init__()\n        assert (d_out % num_heads == 0), \\\n            \"d_out must be divisible by num_heads\"\n\n        self.d_out = d_out\n        self.num_heads = num_heads\n        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer(\n            \"mask\",\n            torch.triu(torch.ones(context_length, context_length),\n                       diagonal=1)\n        )\n\n    def forward(self, x):\n        b, num_tokens, d_in = x.shape\n\n        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n        queries = self.W_query(x)\n        values = self.W_value(x)\n\n        # We implicitly split the matrix by adding a `num_heads` dimension\n        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n\n        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n        keys = keys.transpose(1, 2)\n        queries = queries.transpose(1, 2)\n        values = values.transpose(1, 2)\n\n        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n\n        # Original mask truncated to the number of tokens and converted to boolean\n        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n\n        # Use the mask to fill attention scores\n        attn_scores.masked_fill_(mask_bool, -torch.inf)\n\n        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n\n        # Shape: (b, num_tokens, num_heads, head_dim)\n        context_vec = (attn_weights @ values).transpose(1, 2)\n\n        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n        context_vec = self.out_proj(context_vec) # optional projection\n\n        return context_vec","metadata":{"id":"58d28c8b-0132-456d-a72c-85973ad6b4bb","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T12:36:23.270041Z","iopub.execute_input":"2025-05-18T12:36:23.270561Z","iopub.status.idle":"2025-05-18T12:36:23.279227Z","shell.execute_reply.started":"2025-05-18T12:36:23.270541Z","shell.execute_reply":"2025-05-18T12:36:23.278172Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Layer Norm","metadata":{"id":"118798fd-f29c-4cf1-a4c5-479fe6d06470"}},{"cell_type":"code","source":"class LayerNorm(nn.Module):\n    def __init__(self, emb_dim):\n        super().__init__()\n        self.eps = 1e-5\n        self.scale = nn.Parameter(torch.ones(emb_dim))\n        self.shift = nn.Parameter(torch.zeros(emb_dim))\n\n    def forward(self, x):\n        mean = x.mean(dim=-1, keepdim=True)\n        var = x.var(dim=-1, keepdim=True, unbiased=False)\n        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n        return self.scale * norm_x + self.shift","metadata":{"id":"c75aa4aa-29f8-4e95-86bd-55a0c472c7ef","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T12:36:23.280393Z","iopub.execute_input":"2025-05-18T12:36:23.280670Z","iopub.status.idle":"2025-05-18T12:36:23.311999Z","shell.execute_reply.started":"2025-05-18T12:36:23.280645Z","shell.execute_reply":"2025-05-18T12:36:23.311192Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Gelu Activation","metadata":{"id":"07d2db1a-709d-4e1c-804c-ec13e4cdbd1b"}},{"cell_type":"code","source":"class GELU(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return 0.5 * x * (1 + torch.tanh(\n            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n            (x + 0.044715 * torch.pow(x, 3))\n        ))","metadata":{"id":"49c4a17d-8886-4b9e-a771-a6120b99b24e","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T12:36:23.313696Z","iopub.execute_input":"2025-05-18T12:36:23.313896Z","iopub.status.idle":"2025-05-18T12:36:23.324786Z","shell.execute_reply.started":"2025-05-18T12:36:23.313881Z","shell.execute_reply":"2025-05-18T12:36:23.324069Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## FFN","metadata":{"id":"c79589b7-2378-4401-8fad-e1105277056f"}},{"cell_type":"code","source":"class FeedForward(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), ## Expansion\n            GELU(), ## Activation\n            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), ## Contraction\n        )\n\n    def forward(self, x):\n        return self.layers(x)","metadata":{"id":"53d07f2d-e932-4039-98fa-6025cdc92bf3","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T12:36:23.325577Z","iopub.execute_input":"2025-05-18T12:36:23.325974Z","iopub.status.idle":"2025-05-18T12:36:23.342155Z","shell.execute_reply.started":"2025-05-18T12:36:23.325956Z","shell.execute_reply":"2025-05-18T12:36:23.341475Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Transformer block","metadata":{"id":"c47e05ac-4c6e-40b0-bff2-cb0b3b051fe6"}},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.att = MultiHeadAttention(\n            d_in=cfg[\"emb_dim\"],\n            d_out=cfg[\"emb_dim\"],\n            context_length=cfg[\"context_length\"],\n            num_heads=cfg[\"n_heads\"],\n            dropout=cfg[\"drop_rate\"],\n            qkv_bias=cfg[\"qkv_bias\"])\n        self.ff = FeedForward(cfg)\n        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n\n    def forward(self, x):\n        # Shortcut connection for attention block\n        shortcut = x\n        x = self.norm1(x)\n        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n        x = self.drop_shortcut(x)\n        x = x + shortcut  # Add the original input back\n\n        # Shortcut connection for feed forward block\n        shortcut = x\n        x = self.norm2(x)\n        x = self.ff(x)\n        # 2*4*768\n        x = self.drop_shortcut(x)\n        x = x + shortcut  # Add the original input back\n\n        return x","metadata":{"id":"b4ba4f7f-febb-4758-bab9-2879815d747a","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T12:36:23.342939Z","iopub.execute_input":"2025-05-18T12:36:23.343217Z","iopub.status.idle":"2025-05-18T12:36:23.351933Z","shell.execute_reply.started":"2025-05-18T12:36:23.343192Z","shell.execute_reply":"2025-05-18T12:36:23.351188Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## GPT Model","metadata":{"id":"9c2dd9ea-3fc5-41c3-9a9c-0a32ac035eec"}},{"cell_type":"code","source":"class GPTModel(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n\n        self.trf_blocks = nn.Sequential(\n            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n\n        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n        self.out_head = nn.Linear(\n            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n        )\n\n    def forward(self, in_idx):\n        batch_size, seq_len = in_idx.shape\n        tok_embeds = self.tok_emb(in_idx)\n        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n        x = self.drop_emb(x)\n        x = self.trf_blocks(x)\n        x = self.final_norm(x)\n        logits = self.out_head(x)\n        return logits","metadata":{"id":"a59f6721-6115-4fed-9008-47c3db482831","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T12:36:23.352775Z","iopub.execute_input":"2025-05-18T12:36:23.353098Z","iopub.status.idle":"2025-05-18T12:36:23.368291Z","shell.execute_reply.started":"2025-05-18T12:36:23.353074Z","shell.execute_reply":"2025-05-18T12:36:23.367547Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Load pretrained model weights","metadata":{"id":"yi9tGCRhCrs6"}},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom transformers import GPT2LMHeadModel\nfoundational_model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\nstate_dict = foundational_model.state_dict()","metadata":{"id":"Onl0FAGdCw4k","outputId":"8de1884e-283f-400e-f23a-60cb4b4a1baa","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T12:36:23.369147Z","iopub.execute_input":"2025-05-18T12:36:23.369448Z","iopub.status.idle":"2025-05-18T12:36:32.058232Z","shell.execute_reply.started":"2025-05-18T12:36:23.369422Z","shell.execute_reply":"2025-05-18T12:36:32.057538Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d49e66f7bd7243828d24b7d03a234ce4"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"020a740ea02a40d7ba5c9d468388d2fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09a74b93b33645d3b7090b0eea73bb72"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"def assign(left, right):\n    if left.shape != right.shape:\n        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n    return torch.nn.Parameter(torch.tensor(right))\ndef load_weights(n_layers,gpt2):\n    gpt2.tok_emb.weight = assign(gpt2.tok_emb.weight, (state_dict['transformer.wte.weight']))\n    gpt2.pos_emb.weight = assign(gpt2.pos_emb.weight, (state_dict['transformer.wpe.weight']))\n    for i in range(n_layers):\n        gpt2.trf_blocks[i].norm1.scale = assign(gpt2.trf_blocks[i].norm1.scale,(state_dict[f'transformer.h.{i}.ln_1.weight']))\n        gpt2.trf_blocks[i].norm1.shift = assign(gpt2.trf_blocks[i].norm1.shift,(state_dict[f'transformer.h.{i}.ln_1.bias']))\n\n        q_w, k_w, v_w = np.split(state_dict[f'transformer.h.{i}.attn.c_attn.weight'],3,axis=-1)\n        q_b, k_b, v_b = np.split(state_dict[f'transformer.h.{i}.attn.c_attn.bias'],3,axis=-1)\n        gpt2.trf_blocks[i].att.W_query.weight = assign(gpt2.trf_blocks[i].att.W_query.weight,(q_w.T))\n        gpt2.trf_blocks[i].att.W_key.weight = assign(gpt2.trf_blocks[i].att.W_key.weight, (k_w.T))\n        gpt2.trf_blocks[i].att.W_value.weight = assign(gpt2.trf_blocks[i].att.W_value.weight, (v_w.T))\n        gpt2.trf_blocks[i].att.W_query.bias = assign(gpt2.trf_blocks[i].att.W_query.bias, (q_b))\n        gpt2.trf_blocks[i].att.W_key.bias = assign(gpt2.trf_blocks[i].att.W_key.bias, (k_b))\n        gpt2.trf_blocks[i].att.W_value.bias = assign(gpt2.trf_blocks[i].att.W_value.bias, (v_b))\n\n        gpt2.trf_blocks[i].att.out_proj.weight = assign(gpt2.trf_blocks[i].att.out_proj.weight, (state_dict[f'transformer.h.{i}.attn.c_proj.weight']).T)\n        gpt2.trf_blocks[i].att.out_proj.bias = assign(gpt2.trf_blocks[i].att.out_proj.bias, (state_dict[f'transformer.h.{i}.attn.c_proj.bias']))\n\n        gpt2.trf_blocks[i].norm2.scale = assign(gpt2.trf_blocks[i].norm2.scale,(state_dict[f'transformer.h.{i}.ln_2.weight']))\n        gpt2.trf_blocks[i].norm2.bias = assign(gpt2.trf_blocks[i].norm2.scale,(state_dict[f'transformer.h.{i}.ln_2.bias']))\n\n        gpt2.trf_blocks[i].ff.layers[0].weight = assign(gpt2.trf_blocks[i].ff.layers[0].weight,(state_dict[f'transformer.h.{i}.mlp.c_fc.weight'].T))\n        gpt2.trf_blocks[i].ff.layers[0].bias = assign(gpt2.trf_blocks[i].ff.layers[0].bias,(state_dict[f'transformer.h.{i}.mlp.c_fc.bias']))\n        gpt2.trf_blocks[i].ff.layers[2].weight = assign(gpt2.trf_blocks[i].ff.layers[2].weight,(state_dict[f'transformer.h.{i}.mlp.c_proj.weight'].T))\n        gpt2.trf_blocks[i].ff.layers[2].bias = assign(gpt2.trf_blocks[i].ff.layers[2].bias,(state_dict[f'transformer.h.{i}.mlp.c_proj.bias']))\n\n    gpt2.final_norm.scale = assign(gpt2.final_norm.scale,(state_dict['transformer.ln_f.weight']))\n    gpt2.final_norm.shift = assign(gpt2.final_norm.shift,(state_dict['transformer.ln_f.bias']))\n    gpt2.out_head.weight = assign(gpt2.out_head.weight, (state_dict['lm_head.weight']))","metadata":{"id":"bVGX3XotCp7O","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T12:36:32.058888Z","iopub.execute_input":"2025-05-18T12:36:32.059161Z","iopub.status.idle":"2025-05-18T12:36:32.069455Z","shell.execute_reply.started":"2025-05-18T12:36:32.059119Z","shell.execute_reply":"2025-05-18T12:36:32.068712Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"BASE_CONFIG = {\n    \"vocab_size\": 50257,     # Vocabulary size\n    \"context_length\": 1024,  # Context length\n    \"drop_rate\": 0.1,        # Dropout rate\n    \"qkv_bias\": True         # Query-key-value bias\n}\n\nmodel_configs = {\n    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n}\nBASE_CONFIG.update(model_configs[\"gpt2-medium (355M)\"])\nmodel = GPTModel(BASE_CONFIG)\nload_weights(BASE_CONFIG[\"n_layers\"],model)","metadata":{"id":"Pglu6kBVDMoS","outputId":"cf60efb9-1b95-4562-9798-5ad2cf0b4500","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T12:36:36.489289Z","iopub.execute_input":"2025-05-18T12:36:36.489545Z","iopub.status.idle":"2025-05-18T12:36:40.703636Z","shell.execute_reply.started":"2025-05-18T12:36:36.489525Z","shell.execute_reply":"2025-05-18T12:36:40.703042Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/2843188208.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return torch.nn.Parameter(torch.tensor(right))\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"params = sum(p.numel() for p in model.parameters())\nprint(f\"Total number of parameters: {params}\")","metadata":{"id":"lLU_adFUN72D","outputId":"26269e55-7c32-4088-cab7-45692abd9d15","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T12:36:40.704730Z","iopub.execute_input":"2025-05-18T12:36:40.704914Z","iopub.status.idle":"2025-05-18T12:36:40.833663Z","shell.execute_reply.started":"2025-05-18T12:36:40.704899Z","shell.execute_reply":"2025-05-18T12:36:40.832965Z"}},"outputs":[{"name":"stdout","text":"Total number of parameters: 406310912\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"with open('/kaggle/input/small-instruction-data/instruction-data.json', 'r') as f:\n    data = json.load(f)","metadata":{"id":"W6WDxRmEzNxg","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T12:37:07.068534Z","iopub.execute_input":"2025-05-18T12:37:07.069149Z","iopub.status.idle":"2025-05-18T12:37:07.084344Z","shell.execute_reply.started":"2025-05-18T12:37:07.069100Z","shell.execute_reply":"2025-05-18T12:37:07.083836Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"len(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T12:37:10.476150Z","iopub.execute_input":"2025-05-18T12:37:10.476696Z","iopub.status.idle":"2025-05-18T12:37:10.480792Z","shell.execute_reply.started":"2025-05-18T12:37:10.476674Z","shell.execute_reply":"2025-05-18T12:37:10.480240Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"1100"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"## Convert Input text to alpaca format","metadata":{"id":"Q5TqLzwu2cmP"}},{"cell_type":"code","source":"def format_input(entry):\n    instruction_text = (\n        f\"Below is an instruction that describes a task. \"\n        f\"Write a response that appropriately completes the request.\"\n        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n    )\n\n    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n\n    return instruction_text + input_text","metadata":{"id":"453S8HN-zofD","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T12:37:19.659436Z","iopub.execute_input":"2025-05-18T12:37:19.659773Z","iopub.status.idle":"2025-05-18T12:37:19.663749Z","shell.execute_reply.started":"2025-05-18T12:37:19.659741Z","shell.execute_reply":"2025-05-18T12:37:19.663180Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"train_portion = int(len(data) * 0.85)  # 85% for training\ntest_portion = int(len(data) * 0.1)    # 10% for testing\nval_portion = len(data) - train_portion - test_portion  # Remaining 5% for validation\n\ntrain_data = data[:train_portion]\ntest_data = data[train_portion:train_portion + test_portion]\nval_data = data[train_portion + test_portion:]","metadata":{"id":"gEWjSikd2x0o","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T12:37:21.814448Z","iopub.execute_input":"2025-05-18T12:37:21.814712Z","iopub.status.idle":"2025-05-18T12:37:21.819057Z","shell.execute_reply.started":"2025-05-18T12:37:21.814692Z","shell.execute_reply":"2025-05-18T12:37:21.818341Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class InstructionDataset(Dataset):\n    def __init__(self, data, tokenizer):\n        self.data = data\n\n        # Pre-tokenize texts\n        self.encoded_texts = []\n        for entry in data:\n            instruction_plus_input = format_input(entry)\n            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n            full_text = instruction_plus_input + response_text\n            self.encoded_texts.append(\n                tokenizer.encode(full_text)\n            )\n\n    def __getitem__(self, index):\n        return self.encoded_texts[index]\n\n    def __len__(self):\n        return len(self.data)","metadata":{"id":"v2buJcy620qE","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T12:37:23.543394Z","iopub.execute_input":"2025-05-18T12:37:23.543933Z","iopub.status.idle":"2025-05-18T12:37:23.548460Z","shell.execute_reply.started":"2025-05-18T12:37:23.543912Z","shell.execute_reply":"2025-05-18T12:37:23.547655Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def custom_collate_fn(\n    batch,\n    pad_token_id=50256,\n    ignore_index=-100,\n    allowed_max_length=None,\n    device=\"cpu\"\n):\n    # Find the longest sequence in the batch\n    batch_max_length = max(len(item)+1 for item in batch)\n\n    # Pad and prepare inputs and targets\n    inputs_lst, targets_lst = [], []\n\n    for item in batch:\n        new_item = item.copy()\n        # Add an <|endoftext|> token\n        new_item += [pad_token_id]\n        # Pad sequences to max_length\n        padded = (\n            new_item + [pad_token_id] *\n            (batch_max_length - len(new_item))\n        )\n        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n\n        # New: Replace all but the first padding tokens in targets by ignore_index\n        mask = targets == pad_token_id\n        indices = torch.nonzero(mask).squeeze()\n        if indices.numel() > 1:\n            targets[indices[1:]] = ignore_index\n\n        # New: Optionally truncate to maximum sequence length\n        if allowed_max_length is not None:\n            inputs = inputs[:allowed_max_length]\n            targets = targets[:allowed_max_length]\n\n        inputs_lst.append(inputs)\n        targets_lst.append(targets)\n\n    # Convert list of inputs and targets to tensors and transfer to target device\n    inputs_tensor = torch.stack(inputs_lst).to(device)\n    targets_tensor = torch.stack(targets_lst).to(device)\n\n    return inputs_tensor, targets_tensor","metadata":{"id":"_UuyLob0329u","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T12:37:26.305620Z","iopub.execute_input":"2025-05-18T12:37:26.305912Z","iopub.status.idle":"2025-05-18T12:37:26.313215Z","shell.execute_reply.started":"2025-05-18T12:37:26.305892Z","shell.execute_reply":"2025-05-18T12:37:26.312382Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"from functools import partial\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ncustomized_collate_fn = partial(custom_collate_fn, device=device, allowed_max_length=1024)","metadata":{"id":"KPnvPYMlBB3Y","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T12:37:35.350197Z","iopub.execute_input":"2025-05-18T12:37:35.350925Z","iopub.status.idle":"2025-05-18T12:37:35.354651Z","shell.execute_reply.started":"2025-05-18T12:37:35.350901Z","shell.execute_reply":"2025-05-18T12:37:35.353947Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"tokenizer = tiktoken.get_encoding(\"gpt2\")\nnum_workers = 0\nbatch_size = 4\n\ntorch.manual_seed(123)\n\ntrain_dataset = InstructionDataset(train_data, tokenizer)\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    collate_fn=customized_collate_fn,\n    shuffle=True,\n    drop_last=True,\n    num_workers=num_workers\n)\n\nval_dataset = InstructionDataset(val_data, tokenizer)\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=batch_size,\n    collate_fn=customized_collate_fn,\n    shuffle=False,\n    drop_last=False,\n    num_workers=num_workers\n)\n\ntest_dataset = InstructionDataset(test_data, tokenizer)\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=batch_size,\n    collate_fn=customized_collate_fn,\n    shuffle=False,\n    drop_last=False,\n    num_workers=num_workers\n)","metadata":{"id":"dPtYmGscJ786","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T12:37:40.293096Z","iopub.execute_input":"2025-05-18T12:37:40.293387Z","iopub.status.idle":"2025-05-18T12:37:41.505391Z","shell.execute_reply.started":"2025-05-18T12:37:40.293366Z","shell.execute_reply":"2025-05-18T12:37:41.504820Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def text_to_token_ids(text, tokenizer):\n    encoded = tokenizer.encode(text,allowed_special={'<|endoftext|>'})\n    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n    return encoded_tensor\n\ndef token_ids_to_text(token_ids, tokenizer):\n    flat = token_ids.squeeze(0) # remove batch dimension\n    return tokenizer.decode(flat.tolist())\n\ndef generate_text_simple(model, idx, max_new_tokens, context_size):\n    # idx is (batch, n_tokens) array of indices in the current context\n\n    ###Input batch:\n ###tensor([[6109, 3626, 6100,  345],\n        ##[6109, 1110, 6622,  257]])\n\n    for _ in range(max_new_tokens):\n\n        # Crop current context if it exceeds the supported context size\n        # E.g., if LLM supports only 5 tokens, and the context size is 10\n        # then only the last 5 tokens are used as context\n        idx_cond = idx[:, -context_size:]\n\n        # Get the predictions\n        with torch.no_grad():\n            logits = model(idx_cond) ### batch, n_tokens, vocab_size\n\n        # Focus only on the last time step\n        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n        logits = logits[:, -1, :]\n\n        # Apply softmax to get probabilities\n        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n\n        # Get the idx of the vocab entry with the highest probability value\n        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n\n        # Append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n\n    return idx\ndef generate_and_print_sample(model, tokenizer, device, start_context):\n    model.eval()\n    context_size = model.pos_emb.weight.shape[0]\n    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n    with torch.no_grad():\n        token_ids = generate_text_simple(\n            model=model, idx=encoded,\n            max_new_tokens=50, context_size=context_size\n        )\n    decoded_text = token_ids_to_text(token_ids, tokenizer)\n    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n    model.train()\ndef evaluate_model(model, train_loader, val_loader, device, eval_iter):\n    model.eval()\n    with torch.no_grad():\n        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n    model.train()\n    return train_loss, val_loss\ndef calc_loss_batch(input_batch, target_batch, model, device):\n    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n    logits = model(input_batch)\n    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n    return loss\n\n\ndef calc_loss_loader(data_loader, model, device, num_batches=None):\n    total_loss = 0.\n    if len(data_loader) == 0:\n        return float(\"nan\")\n    elif num_batches is None:\n        num_batches = len(data_loader)\n    else:\n        # Reduce the number of batches to match the total number of batches in the data loader\n        # if num_batches exceeds the number of batches in the data loader\n        num_batches = min(num_batches, len(data_loader))\n    for i, (input_batch, target_batch) in enumerate(data_loader):\n        if i < num_batches:\n            loss = calc_loss_batch(input_batch, target_batch, model, device)\n            total_loss += loss.item()\n        else:\n            break\n    return total_loss / num_batches\n\ndef train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n                       eval_freq, eval_iter, start_context, tokenizer):\n    # Initialize lists to track losses and tokens seen\n    train_losses, val_losses, track_tokens_seen = [], [], []\n    tokens_seen, global_step = 0, -1\n\n    # Main training loop\n    for epoch in range(num_epochs):\n        model.train()  # Set model to training mode\n\n        for input_batch, target_batch in train_loader:\n            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n            loss = calc_loss_batch(input_batch, target_batch, model, device)\n            loss.backward() # Calculate loss gradients\n            optimizer.step() # Update model weights using loss gradients\n            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n            global_step += 1\n\n            # Optional evaluation step\n            if global_step % eval_freq == 0:\n                train_loss, val_loss = evaluate_model(\n                    model, train_loader, val_loader, device, eval_iter)\n                train_losses.append(train_loss)\n                val_losses.append(val_loss)\n                track_tokens_seen.append(tokens_seen)\n                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n        torch.cuda.empty_cache()\n\n\n        # Print a sample text after each epoch\n        generate_and_print_sample(\n            model, tokenizer, device, start_context\n        )\n\n    return train_losses, val_losses, track_tokens_seen","metadata":{"id":"JRqdd0M8LY7Y","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T12:37:51.934918Z","iopub.execute_input":"2025-05-18T12:37:51.935695Z","iopub.status.idle":"2025-05-18T12:37:51.954406Z","shell.execute_reply.started":"2025-05-18T12:37:51.935657Z","shell.execute_reply":"2025-05-18T12:37:51.953540Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T12:38:19.905278Z","iopub.execute_input":"2025-05-18T12:38:19.905751Z","iopub.status.idle":"2025-05-18T12:38:19.910225Z","shell.execute_reply.started":"2025-05-18T12:38:19.905727Z","shell.execute_reply":"2025-05-18T12:38:19.909591Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"import time\n\nstart_time = time.time()\n\ntorch.manual_seed(123)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\nnum_epochs = 5\nmodel.to(device)\ntrain_losses, val_losses, tokens_seen = train_model_simple(\n    model, train_loader, val_loader, optimizer, device,\n    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n    start_context=format_input(val_data[0]), tokenizer=tokenizer\n)\n\nend_time = time.time()\nexecution_time_minutes = (end_time - start_time) / 60\nprint(f\"Training completed in {execution_time_minutes:.2f} minutes.\")","metadata":{"id":"Ce-9B3X4LdZt","outputId":"2c056e8e-58f1-4282-e80a-9f5c8c8cbf87","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T12:38:37.309028Z","iopub.execute_input":"2025-05-18T12:38:37.309721Z","iopub.status.idle":"2025-05-18T12:48:20.572703Z","shell.execute_reply.started":"2025-05-18T12:38:37.309696Z","shell.execute_reply":"2025-05-18T12:48:20.571837Z"}},"outputs":[{"name":"stdout","text":"Ep 1 (Step 000000): Train loss 3.639, Val loss 3.475\nEp 1 (Step 000005): Train loss 1.753, Val loss 1.673\nEp 1 (Step 000010): Train loss 1.048, Val loss 1.248\nEp 1 (Step 000015): Train loss 1.022, Val loss 1.139\nEp 1 (Step 000020): Train loss 0.900, Val loss 1.078\nEp 1 (Step 000025): Train loss 0.880, Val loss 1.042\nEp 1 (Step 000030): Train loss 0.853, Val loss 1.026\nEp 1 (Step 000035): Train loss 1.009, Val loss 1.027\nEp 1 (Step 000040): Train loss 0.884, Val loss 1.004\nEp 1 (Step 000045): Train loss 0.750, Val loss 0.980\nEp 1 (Step 000050): Train loss 0.831, Val loss 0.963\nEp 1 (Step 000055): Train loss 0.852, Val loss 0.964\nEp 1 (Step 000060): Train loss 0.870, Val loss 0.955\nEp 1 (Step 000065): Train loss 0.787, Val loss 0.940\nEp 1 (Step 000070): Train loss 0.666, Val loss 0.924\nEp 1 (Step 000075): Train loss 0.669, Val loss 0.911\nEp 1 (Step 000080): Train loss 0.730, Val loss 0.911\nEp 1 (Step 000085): Train loss 0.676, Val loss 0.911\nEp 1 (Step 000090): Train loss 0.688, Val loss 0.900\nEp 1 (Step 000095): Train loss 0.596, Val loss 0.895\nEp 1 (Step 000100): Train loss 0.614, Val loss 0.877\nEp 1 (Step 000105): Train loss 0.739, Val loss 0.863\nEp 1 (Step 000110): Train loss 0.748, Val loss 0.850\nEp 1 (Step 000115): Train loss 0.644, Val loss 0.844\nEp 1 (Step 000120): Train loss 0.777, Val loss 0.833\nEp 1 (Step 000125): Train loss 0.641, Val loss 0.832\nEp 1 (Step 000130): Train loss 0.667, Val loss 0.823\nEp 1 (Step 000135): Train loss 0.553, Val loss 0.820\nEp 1 (Step 000140): Train loss 0.552, Val loss 0.822\nEp 1 (Step 000145): Train loss 0.605, Val loss 0.819\nEp 1 (Step 000150): Train loss 0.614, Val loss 0.820\nEp 1 (Step 000155): Train loss 0.561, Val loss 0.816\nEp 1 (Step 000160): Train loss 0.533, Val loss 0.800\nEp 1 (Step 000165): Train loss 0.733, Val loss 0.795\nEp 1 (Step 000170): Train loss 0.592, Val loss 0.805\nEp 1 (Step 000175): Train loss 0.549, Val loss 0.782\nEp 1 (Step 000180): Train loss 0.624, Val loss 0.775\nEp 1 (Step 000185): Train loss 0.518, Val loss 0.774\nEp 1 (Step 000190): Train loss 0.647, Val loss 0.774\nEp 1 (Step 000195): Train loss 0.497, Val loss 0.761\nEp 1 (Step 000200): Train loss 0.653, Val loss 0.760\nEp 1 (Step 000205): Train loss 0.545, Val loss 0.758\nEp 1 (Step 000210): Train loss 0.624, Val loss 0.765\nEp 1 (Step 000215): Train loss 0.422, Val loss 0.757\nEp 1 (Step 000220): Train loss 0.491, Val loss 0.742\nEp 1 (Step 000225): Train loss 0.461, Val loss 0.732\nEp 1 (Step 000230): Train loss 0.575, Val loss 0.727\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is prepared every day by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive:\nEp 2 (Step 000235): Train loss 0.570, Val loss 0.723\nEp 2 (Step 000240): Train loss 0.437, Val loss 0.722\nEp 2 (Step 000245): Train loss 0.531, Val loss 0.728\nEp 2 (Step 000250): Train loss 0.418, Val loss 0.734\nEp 2 (Step 000255): Train loss 0.438, Val loss 0.739\nEp 2 (Step 000260): Train loss 0.510, Val loss 0.742\nEp 2 (Step 000265): Train loss 0.405, Val loss 0.746\nEp 2 (Step 000270): Train loss 0.457, Val loss 0.738\nEp 2 (Step 000275): Train loss 0.579, Val loss 0.727\nEp 2 (Step 000280): Train loss 0.521, Val loss 0.727\nEp 2 (Step 000285): Train loss 0.474, Val loss 0.732\nEp 2 (Step 000290): Train loss 0.340, Val loss 0.742\nEp 2 (Step 000295): Train loss 0.431, Val loss 0.743\nEp 2 (Step 000300): Train loss 0.422, Val loss 0.754\nEp 2 (Step 000305): Train loss 0.540, Val loss 0.753\nEp 2 (Step 000310): Train loss 0.440, Val loss 0.752\nEp 2 (Step 000315): Train loss 0.470, Val loss 0.756\nEp 2 (Step 000320): Train loss 0.394, Val loss 0.762\nEp 2 (Step 000325): Train loss 0.507, Val loss 0.750\nEp 2 (Step 000330): Train loss 0.468, Val loss 0.745\nEp 2 (Step 000335): Train loss 0.420, Val loss 0.741\nEp 2 (Step 000340): Train loss 0.488, Val loss 0.750\nEp 2 (Step 000345): Train loss 0.403, Val loss 0.745\nEp 2 (Step 000350): Train loss 0.383, Val loss 0.751\nEp 2 (Step 000355): Train loss 0.447, Val loss 0.754\nEp 2 (Step 000360): Train loss 0.368, Val loss 0.754\nEp 2 (Step 000365): Train loss 0.344, Val loss 0.745\nEp 2 (Step 000370): Train loss 0.490, Val loss 0.728\nEp 2 (Step 000375): Train loss 0.430, Val loss 0.725\nEp 2 (Step 000380): Train loss 0.423, Val loss 0.727\nEp 2 (Step 000385): Train loss 0.471, Val loss 0.730\nEp 2 (Step 000390): Train loss 0.409, Val loss 0.721\nEp 2 (Step 000395): Train loss 0.473, Val loss 0.722\nEp 2 (Step 000400): Train loss 0.409, Val loss 0.722\nEp 2 (Step 000405): Train loss 0.430, Val loss 0.722\nEp 2 (Step 000410): Train loss 0.401, Val loss 0.721\nEp 2 (Step 000415): Train loss 0.417, Val loss 0.716\nEp 2 (Step 000420): Train loss 0.385, Val loss 0.714\nEp 2 (Step 000425): Train loss 0.417, Val loss 0.707\nEp 2 (Step 000430): Train loss 0.372, Val loss 0.704\nEp 2 (Step 000435): Train loss 0.334, Val loss 0.702\nEp 2 (Step 000440): Train loss 0.395, Val loss 0.701\nEp 2 (Step 000445): Train loss 0.396, Val loss 0.701\nEp 2 (Step 000450): Train loss 0.355, Val loss 0.705\nEp 2 (Step 000455): Train loss 0.359, Val loss 0.709\nEp 2 (Step 000460): Train loss 0.395, Val loss 0.721\nEp 2 (Step 000465): Train loss 0.355, Val loss 0.730\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal was prepared by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: What is the capital of Indonesia?  ###\nEp 3 (Step 000470): Train loss 0.334, Val loss 0.726\nEp 3 (Step 000475): Train loss 0.306, Val loss 0.716\nEp 3 (Step 000480): Train loss 0.310, Val loss 0.707\nEp 3 (Step 000485): Train loss 0.283, Val loss 0.709\nEp 3 (Step 000490): Train loss 0.329, Val loss 0.715\nEp 3 (Step 000495): Train loss 0.299, Val loss 0.724\nEp 3 (Step 000500): Train loss 0.371, Val loss 0.734\nEp 3 (Step 000505): Train loss 0.298, Val loss 0.737\nEp 3 (Step 000510): Train loss 0.356, Val loss 0.728\nEp 3 (Step 000515): Train loss 0.295, Val loss 0.729\nEp 3 (Step 000520): Train loss 0.290, Val loss 0.724\nEp 3 (Step 000525): Train loss 0.323, Val loss 0.735\nEp 3 (Step 000530): Train loss 0.286, Val loss 0.746\nEp 3 (Step 000535): Train loss 0.304, Val loss 0.760\nEp 3 (Step 000540): Train loss 0.300, Val loss 0.755\nEp 3 (Step 000545): Train loss 0.314, Val loss 0.749\nEp 3 (Step 000550): Train loss 0.300, Val loss 0.745\nEp 3 (Step 000555): Train loss 0.273, Val loss 0.742\nEp 3 (Step 000560): Train loss 0.281, Val loss 0.739\nEp 3 (Step 000565): Train loss 0.297, Val loss 0.740\nEp 3 (Step 000570): Train loss 0.280, Val loss 0.745\nEp 3 (Step 000575): Train loss 0.318, Val loss 0.758\nEp 3 (Step 000580): Train loss 0.301, Val loss 0.762\nEp 3 (Step 000585): Train loss 0.271, Val loss 0.750\nEp 3 (Step 000590): Train loss 0.275, Val loss 0.743\nEp 3 (Step 000595): Train loss 0.335, Val loss 0.741\nEp 3 (Step 000600): Train loss 0.349, Val loss 0.741\nEp 3 (Step 000605): Train loss 0.283, Val loss 0.751\nEp 3 (Step 000610): Train loss 0.289, Val loss 0.753\nEp 3 (Step 000615): Train loss 0.274, Val loss 0.746\nEp 3 (Step 000620): Train loss 0.310, Val loss 0.739\nEp 3 (Step 000625): Train loss 0.292, Val loss 0.735\nEp 3 (Step 000630): Train loss 0.314, Val loss 0.733\nEp 3 (Step 000635): Train loss 0.291, Val loss 0.739\nEp 3 (Step 000640): Train loss 0.272, Val loss 0.751\nEp 3 (Step 000645): Train loss 0.337, Val loss 0.748\nEp 3 (Step 000650): Train loss 0.260, Val loss 0.740\nEp 3 (Step 000655): Train loss 0.290, Val loss 0.737\nEp 3 (Step 000660): Train loss 0.280, Val loss 0.739\nEp 3 (Step 000665): Train loss 0.262, Val loss 0.734\nEp 3 (Step 000670): Train loss 0.241, Val loss 0.735\nEp 3 (Step 000675): Train loss 0.304, Val loss 0.728\nEp 3 (Step 000680): Train loss 0.300, Val loss 0.729\nEp 3 (Step 000685): Train loss 0.260, Val loss 0.727\nEp 3 (Step 000690): Train loss 0.279, Val loss 0.720\nEp 3 (Step 000695): Train loss 0.267, Val loss 0.713\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is prepared daily by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: '\nEp 4 (Step 000700): Train loss 0.275, Val loss 0.710\nEp 4 (Step 000705): Train loss 0.254, Val loss 0.721\nEp 4 (Step 000710): Train loss 0.230, Val loss 0.735\nEp 4 (Step 000715): Train loss 0.300, Val loss 0.766\nEp 4 (Step 000720): Train loss 0.265, Val loss 0.771\nEp 4 (Step 000725): Train loss 0.240, Val loss 0.766\nEp 4 (Step 000730): Train loss 0.241, Val loss 0.771\nEp 4 (Step 000735): Train loss 0.247, Val loss 0.767\nEp 4 (Step 000740): Train loss 0.267, Val loss 0.767\nEp 4 (Step 000745): Train loss 0.238, Val loss 0.752\nEp 4 (Step 000750): Train loss 0.285, Val loss 0.750\nEp 4 (Step 000755): Train loss 0.256, Val loss 0.755\nEp 4 (Step 000760): Train loss 0.268, Val loss 0.761\nEp 4 (Step 000765): Train loss 0.261, Val loss 0.766\nEp 4 (Step 000770): Train loss 0.249, Val loss 0.768\nEp 4 (Step 000775): Train loss 0.245, Val loss 0.748\nEp 4 (Step 000780): Train loss 0.314, Val loss 0.741\nEp 4 (Step 000785): Train loss 0.240, Val loss 0.741\nEp 4 (Step 000790): Train loss 0.246, Val loss 0.741\nEp 4 (Step 000795): Train loss 0.270, Val loss 0.739\nEp 4 (Step 000800): Train loss 0.222, Val loss 0.743\nEp 4 (Step 000805): Train loss 0.222, Val loss 0.744\nEp 4 (Step 000810): Train loss 0.260, Val loss 0.738\nEp 4 (Step 000815): Train loss 0.246, Val loss 0.734\nEp 4 (Step 000820): Train loss 0.227, Val loss 0.742\nEp 4 (Step 000825): Train loss 0.240, Val loss 0.751\nEp 4 (Step 000830): Train loss 0.261, Val loss 0.764\nEp 4 (Step 000835): Train loss 0.226, Val loss 0.765\nEp 4 (Step 000840): Train loss 0.255, Val loss 0.762\nEp 4 (Step 000845): Train loss 0.253, Val loss 0.756\nEp 4 (Step 000850): Train loss 0.254, Val loss 0.762\nEp 4 (Step 000855): Train loss 0.223, Val loss 0.773\nEp 4 (Step 000860): Train loss 0.222, Val loss 0.784\nEp 4 (Step 000865): Train loss 0.220, Val loss 0.781\nEp 4 (Step 000870): Train loss 0.223, Val loss 0.778\nEp 4 (Step 000875): Train loss 0.239, Val loss 0.759\nEp 4 (Step 000880): Train loss 0.223, Val loss 0.754\nEp 4 (Step 000885): Train loss 0.207, Val loss 0.752\nEp 4 (Step 000890): Train loss 0.217, Val loss 0.753\nEp 4 (Step 000895): Train loss 0.219, Val loss 0.746\nEp 4 (Step 000900): Train loss 0.227, Val loss 0.742\nEp 4 (Step 000905): Train loss 0.243, Val loss 0.743\nEp 4 (Step 000910): Train loss 0.246, Val loss 0.740\nEp 4 (Step 000915): Train loss 0.242, Val loss 0.744\nEp 4 (Step 000920): Train loss 0.239, Val loss 0.749\nEp 4 (Step 000925): Train loss 0.228, Val loss 0.745\nEp 4 (Step 000930): Train loss 0.222, Val loss 0.743\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The chef prepares the meal every day.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: What is the chemical symbol for copper?  \nEp 5 (Step 000935): Train loss 0.221, Val loss 0.748\nEp 5 (Step 000940): Train loss 0.212, Val loss 0.760\nEp 5 (Step 000945): Train loss 0.224, Val loss 0.778\nEp 5 (Step 000950): Train loss 0.213, Val loss 0.788\nEp 5 (Step 000955): Train loss 0.226, Val loss 0.806\nEp 5 (Step 000960): Train loss 0.225, Val loss 0.809\nEp 5 (Step 000965): Train loss 0.262, Val loss 0.801\nEp 5 (Step 000970): Train loss 0.250, Val loss 0.790\nEp 5 (Step 000975): Train loss 0.246, Val loss 0.792\nEp 5 (Step 000980): Train loss 0.210, Val loss 0.790\nEp 5 (Step 000985): Train loss 0.210, Val loss 0.790\nEp 5 (Step 000990): Train loss 0.203, Val loss 0.796\nEp 5 (Step 000995): Train loss 0.262, Val loss 0.814\nEp 5 (Step 001000): Train loss 0.232, Val loss 0.827\nEp 5 (Step 001005): Train loss 0.203, Val loss 0.824\nEp 5 (Step 001010): Train loss 0.263, Val loss 0.812\nEp 5 (Step 001015): Train loss 0.242, Val loss 0.790\nEp 5 (Step 001020): Train loss 0.217, Val loss 0.774\nEp 5 (Step 001025): Train loss 0.224, Val loss 0.766\nEp 5 (Step 001030): Train loss 0.234, Val loss 0.772\nEp 5 (Step 001035): Train loss 0.201, Val loss 0.780\nEp 5 (Step 001040): Train loss 0.205, Val loss 0.794\nEp 5 (Step 001045): Train loss 0.246, Val loss 0.800\nEp 5 (Step 001050): Train loss 0.224, Val loss 0.803\nEp 5 (Step 001055): Train loss 0.194, Val loss 0.801\nEp 5 (Step 001060): Train loss 0.241, Val loss 0.799\nEp 5 (Step 001065): Train loss 0.191, Val loss 0.792\nEp 5 (Step 001070): Train loss 0.202, Val loss 0.786\nEp 5 (Step 001075): Train loss 0.196, Val loss 0.789\nEp 5 (Step 001080): Train loss 0.190, Val loss 0.804\nEp 5 (Step 001085): Train loss 0.203, Val loss 0.814\nEp 5 (Step 001090): Train loss 0.222, Val loss 0.814\nEp 5 (Step 001095): Train loss 0.196, Val loss 0.802\nEp 5 (Step 001100): Train loss 0.186, Val loss 0.772\nEp 5 (Step 001105): Train loss 0.230, Val loss 0.756\nEp 5 (Step 001110): Train loss 0.201, Val loss 0.751\nEp 5 (Step 001115): Train loss 0.206, Val loss 0.747\nEp 5 (Step 001120): Train loss 0.230, Val loss 0.743\nEp 5 (Step 001125): Train loss 0.230, Val loss 0.743\nEp 5 (Step 001130): Train loss 0.200, Val loss 0.742\nEp 5 (Step 001135): Train loss 0.176, Val loss 0.750\nEp 5 (Step 001140): Train loss 0.211, Val loss 0.762\nEp 5 (Step 001145): Train loss 0.191, Val loss 0.772\nEp 5 (Step 001150): Train loss 0.205, Val loss 0.772\nEp 5 (Step 001155): Train loss 0.224, Val loss 0.771\nEp 5 (Step 001160): Train loss 0.177, Val loss 0.763\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The chef prepares the meal every day.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Input: The chef cooks the meal every day.  \nTraining completed in 9.72 minutes.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n\n    # For-loop is the same as before: Get logits, and only focus on last time step\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]\n        with torch.no_grad():\n            logits = model(idx_cond)\n        logits = logits[:, -1, :]\n\n        # New: Filter logits with top_k sampling\n        if top_k is not None:\n            # Keep only top_k values\n            top_logits, _ = torch.topk(logits, top_k)\n            min_val = top_logits[:, -1]\n            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n\n        # New: Apply temperature scaling\n        if temperature > 0.0:\n            logits = logits / temperature\n\n            # Apply softmax to get probabilities\n            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n\n            # Sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n\n        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n        else:\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n\n        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n            break\n\n        # Same as before: append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n\n    return idx","metadata":{"id":"zlUf16KkR3GA","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T12:48:39.666184Z","iopub.execute_input":"2025-05-18T12:48:39.666980Z","iopub.status.idle":"2025-05-18T12:48:39.675157Z","shell.execute_reply.started":"2025-05-18T12:48:39.666944Z","shell.execute_reply":"2025-05-18T12:48:39.674441Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"torch.manual_seed(123)\n\n\nfor entry in test_data[:3]:\n\n    input_text = format_input(entry)\n\n    token_ids = generate(\n        model=model,\n        idx=text_to_token_ids(input_text, tokenizer).to(device),\n        max_new_tokens=256,\n        context_size=BASE_CONFIG[\"context_length\"],\n        eos_id=50256\n    )\n    generated_text = token_ids_to_text(token_ids, tokenizer)\n    response_text = (\n        generated_text[len(input_text):]\n        .replace(\"### Response:\", \"\")\n        .strip()\n)\n\n    print(input_text)\n    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n    print(\"-------------------------------------\")","metadata":{"id":"KO0T0bx_UOK9","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T12:48:44.143658Z","iopub.execute_input":"2025-05-18T12:48:44.144224Z","iopub.status.idle":"2025-05-18T12:48:46.131971Z","shell.execute_reply.started":"2025-05-18T12:48:44.144201Z","shell.execute_reply":"2025-05-18T12:48:46.131236Z"}},"outputs":[{"name":"stdout","text":"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nRewrite the sentence using a simile.\n\n### Input:\nThe car is very fast.\n\nCorrect response:\n>> The car is as fast as lightning.\n\nModel response:\n>> The car is as fast as a bullet.\n-------------------------------------\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nWhat type of cloud is typically associated with thunderstorms?\n\nCorrect response:\n>> The type of cloud typically associated with thunderstorms is cumulonimbus.\n\nModel response:\n>> A thunderstorm is a type of intense wind with a speed of up to 300 kilometers per hour.\n-------------------------------------\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nName the author of 'Pride and Prejudice'.\n\nCorrect response:\n>> Jane Austen.\n\nModel response:\n>> The author of 'Pride and Prejudice' is Jane Austen.\n-------------------------------------\n","output_type":"stream"}],"execution_count":24}]}